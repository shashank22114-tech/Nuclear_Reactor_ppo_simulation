# -*- coding: utf-8 -*-
"""Nuclear_Reactor_simulation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G7tKdflBxuQENTm6Kky0QLi_dpIXLn9k
"""

# ==============================================================================
# NUCLEAR REACTOR AUTONOMOUS CONTROL SYSTEM: PPO-KINETICS INTEGRATION
# ==============================================================================
# AUTHOR: N Shashank Reddy
# DATE: 2024
# VERSION: 4.0 (Production / King-of-the-Hill)
#
# DESCRIPTION:
# This script simulates a nuclear reactor core using 6-group point kinetics
# with delayed neutron precursors. It trains an Autonomous Agent (PPO)
# to maintain the reactor at exactly 100% power (King of the Hill scenario).
#
# CRITICAL UPDATES:
# 1. PHYSICS ENGINE: Corrected equilibrium precursor initialization.
# 2. SAFETY LOGIC: Enforces "Do or Die" limits (<50% or >200% Power = SCRAM).
# 3. STABILITY: Includes robust batch protection against instant-crash NaNs.
# ==============================================================================

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import time

# ============================================================
# 1. SYSTEM CONFIGURATION & HYPERPARAMETERS
# ============================================================

class Config:
    # --- Simulation Physics ---
    DT = 0.01                  # Time step: 10ms (High resolution for stability)
    EPISODE_LEN = 2000         # Duration: 20.0 seconds per episode

    # --- Operational Constraints ---
    POWER_TARGET = 1.00        # Normal Operation Point (100%)

    # --- Safety Trip Limits (SCRAM) ---
    MIN_POWER = 0.50           # Low Power Trip (50%) - Prevents "Turtle" strategy
    HARD_LIMIT = 2.00          # High Power Trip (200%) - Prevents Meltdown

    # --- PPO Algorithm Settings ---
    LR_ACTOR = 1e-4            # Learning Rate (Actor) - Conservative
    LR_CRITIC = 5e-4           # Learning Rate (Critic)
    GAMMA = 0.99               # Discount Factor (Future reward weighting)
    GAE_LAMBDA = 0.95          # Generalized Advantage Estimation
    CLIP_EPS = 0.2             # PPO Clipping Range
    UPDATE_EPOCHS = 10         # Gradient steps per batch
    BATCH_SIZE = 64            # Minibatch size
    ENTROPY_COEF = 0.01        # Exploration incentive
    MAX_GRAD_NORM = 0.5        # Gradient clipping to prevent exploding gradients

    # --- Training Loop ---
    N_EPISODES = 100           # Total training episodes
    PRINT_INTERVAL = 20        # Log frequency
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

cfg = Config()

# ============================================================
# 2. PHYSICS ENGINE: 6-GROUP POINT KINETICS
# ============================================================

class ReactorPhysics:
    """
    Simulates the core neutronics and thermal-hydraulics.
    Uses Euler integration for the stiff differential equations of
    neutron population (n) and precursor concentrations (C).
    """
    def __init__(self):
        # Delayed Neutron Parameters (U-235 Thermal Spectrum)
        # Groups: 1 through 6
        self.beta = np.array([0.000215, 0.001424, 0.001274, 0.002568, 0.000748, 0.000273])
        self.lam = np.array([0.0124, 0.0305, 0.111, 0.301, 1.14, 3.01])
        self.beta_total = np.sum(self.beta)
        self.Lambda = 1e-4  # Mean neutron generation time (seconds)

        # Reactivity Feedback Coefficients
        self.alpha_fuel = -1.5e-5  # Doppler Broadening
        self.alpha_cool = -3.0e-5  # Moderator Density

        # Thermal Properties (Lumped Parameter Model)
        self.ha_nominal = 3.0      # Heat transfer coefficient
        self.Cp_f = 20.0           # Fuel heat capacity
        self.Cp_c = 40.0           # Coolant heat capacity

        self.reset()

    def reset(self):
        """
        Resets the reactor to the 'King of the Hill' state:
        Perfectly stable at 100% power.
        """
        self.n = 1.0

        # --- CRITICAL PHYSICS CORRECTION ---
        # Precursor equilibrium concentration C_i0 = (beta_i / (lambda_i * Lambda)) * n0
        self.C = (self.beta / (self.lam * self.Lambda)) * self.n

        # Thermal Equilibrium
        self.Tf = self.n
        self.Tc = 0.8 * self.n

        self.scram = False
        return self.get_state()

    def step(self, action_rod, action_flow):
        """
        Steps the physics simulation forward by DT seconds.
        """
        if self.scram:
            action_rod = -2.0 # Gravity Scram (Rods drop fully)

        # 1. Reactivity Calculation (Delta-k/k)
        # Rod Authority scaled to 0.10$ to represent realistic movement speed
        rho_control = action_rod * self.beta_total * 0.10
        rho_feedback = self.alpha_fuel * (self.Tf - 1.0) + self.alpha_cool * (self.Tc - 0.8)
        rho = rho_control + rho_feedback

        # 2. Point Kinetics Equations
        # dn/dt = (rho - beta)/Lambda * n + Sum(lambda_i * C_i)
        dndt = ((rho - self.beta_total) / self.Lambda) * self.n + np.sum(self.lam * self.C)

        # dC_i/dt = (beta_i / Lambda) * n - lambda_i * C_i
        dCdt = (self.beta / self.Lambda) * self.n - self.lam * self.C

        # 3. Thermal-Hydraulics Equations
        flow = np.clip(action_flow, 0.1, 1.5)
        hA = self.ha_nominal * (flow ** 0.8) # Dittus-Boelter approximation

        P_gen = self.n
        Q_transfer = hA * (self.Tf - self.Tc)
        Q_sink = 2.0 * flow * (self.Tc - 0.5)

        dTf_dt = (P_gen - Q_transfer) / self.Cp_f
        dTc_dt = (Q_transfer - Q_sink) / self.Cp_c

        # 4. Numerical Integration
        self.n += dndt * cfg.DT
        self.C += dCdt * cfg.DT
        self.Tf += dTf_dt * cfg.DT
        self.Tc += dTc_dt * cfg.DT

        # 5. Safety & Validity Checks
        self.n = np.clip(self.n, 0.0, 1e5) # Prevent numerical NaNs

        # SCRAM LOGIC: "The Floor is Lava" + "The Ceiling is Lava"
        if self.n > cfg.HARD_LIMIT or self.n < cfg.MIN_POWER:
            self.scram = True

        return self.get_state()

    def get_state(self):
        """Returns normalized state vector [Power, Avg_Precursors, T_fuel, T_cool]"""
        obs = np.array([self.n, np.mean(self.C), self.Tf, self.Tc], dtype=np.float32)
        return np.nan_to_num(obs, nan=0.0)

# ============================================================
# 3. UTILITIES: ROBUST NORMALIZATION
# ============================================================

class RunningMeanStd:
    """
    Standardizes observations to Mean=0, Variance=1.
    Essential for PPO stability. Includes outlier rejection.
    """
    def __init__(self, shape):
        self.mean = np.zeros(shape)
        self.var = np.ones(shape)
        self.count = 1e-4

    def update(self, x):
        # Reject data from reactor explosions (values > 50.0) to preserve stats
        if np.max(np.abs(x)) > 50: return

        batch_mean = np.mean(x, axis=0)
        batch_var = np.var(x, axis=0)
        batch_count = 1

        delta = batch_mean - self.mean
        tot_count = self.count + batch_count

        new_mean = self.mean + delta * batch_count / tot_count
        m_a = self.var * self.count
        m_b = batch_var * batch_count
        m_2 = m_a + m_b + np.square(delta) * self.count * batch_count / tot_count

        self.mean = new_mean
        self.var = m_2 / tot_count
        self.count = tot_count

    def normalize(self, x):
        return (x - self.mean) / (np.sqrt(self.var) + 1e-8)

# ============================================================
# 4. NEURAL NETWORK: ACTOR-CRITIC ARCHITECTURE
# ============================================================

class ActorCritic(nn.Module):
    def __init__(self, obs_dim, act_dim):
        super().__init__()

        # Shared Feature Extractor
        self.shared = nn.Sequential(
            nn.Linear(obs_dim, 128), nn.Tanh(),
            nn.Linear(128, 128), nn.Tanh()
        )

        # Actor Head (Policy)
        self.actor_mu = nn.Linear(128, act_dim)
        self.actor_logstd = nn.Parameter(torch.zeros(1, act_dim) - 1.0)

        # Critic Head (Value)
        self.critic = nn.Linear(128, 1)

        # NEUTRAL INITIALIZATION
        # Start with 0.0 bias (Hold Steady).
        # We removed the "Safety Bias" (-0.5) so the agent isn't afraid to act.
        with torch.no_grad():
            self.actor_mu.weight.mul_(0.01)
            self.actor_mu.bias.data.fill_(0.0)

    def forward(self, x):
        features = self.shared(x)
        mu = torch.tanh(self.actor_mu(features))
        std = self.actor_logstd.exp().expand_as(mu)
        val = self.critic(features)
        return mu, std, val

    def get_action(self, x):
        mu, std, val = self(x)
        dist = torch.distributions.Normal(mu, std)
        action = dist.sample()
        # Sum log probs for multivariate action space
        return action, dist.log_prob(action).sum(-1), dist.entropy().sum(-1), val

# ============================================================
# 5. MAIN TRAINING LOOP
# ============================================================

def train_reactor():
    # --- Scientific Header ---
    print(f"{'='*70}")
    print(f" NUCLEAR REACTOR AUTONOMOUS CONTROL SIMULATION (PPO-KINETICS)")
    print(f"{'='*70}")
    print(f" SYSTEM SPECIFICATIONS:")
    print(f" [Physics]   Model: 6-Group Point Kinetics (Delayed Neutrons)")
    print(f" [Fuel]      Type: U-235 Thermal Spectrum")
    print(f" [Kinetics]  Beta_eff: {ReactorPhysics().beta_total:.5f} | Lambda: {ReactorPhysics().Lambda} s")
    print(f" [Feedback]  Doppler: -1.5 pcm/K | Moderator: -3.0 pcm/K")
    print(f" [Safety]    Scram Limits: <{cfg.MIN_POWER*100:.0f}% or >{cfg.HARD_LIMIT*100:.0f}%")
    print(f" [Algorithm] Proximal Policy Optimization (Clip={cfg.CLIP_EPS})")
    print(f"{'='*70}\n")

    # Initialization
    physics = ReactorPhysics()
    normalizer = RunningMeanStd(shape=(4,))
    agent = ActorCritic(obs_dim=4, act_dim=2).to(cfg.DEVICE)
    optimizer = optim.Adam(agent.parameters(), lr=cfg.LR_ACTOR)

    history = {'reward': [], 'power': []}

    print(f"{'Ep':>5} | {'Avg Reward':>12} | {'Power (n)':>10} | {'Fuel T':>8} | {'Status':>10}")
    print("-" * 65)

    for episode in range(cfg.N_EPISODES):
        obs_raw = physics.reset()

        # Safety Guard: Bad Initialization
        if not np.isfinite(obs_raw).all(): obs_raw = np.zeros(4)

        normalizer.update(obs_raw)
        obs = normalizer.normalize(obs_raw)

        # PPO Rollout Buffers
        b_obs, b_act, b_logp, b_rew, b_val, b_done = [], [], [], [], [], []
        ep_reward = 0
        done = False

        for step in range(cfg.EPISODE_LEN):
            obs_t = torch.FloatTensor(obs).to(cfg.DEVICE).unsqueeze(0)

            # Safety Guard: NaN Inputs
            if torch.isnan(obs_t).any(): break

            with torch.no_grad():
                action, logp, _, val = agent.get_action(obs_t)

            action_np = action.cpu().numpy()[0]

            # Action Mapping
            # Rods: [-1, 1] mapped to small reactivity insertion inside physics
            act_rod = np.clip(action_np[0], -1.0, 1.0)
            act_flow = np.clip(action_np[1], -1.0, 1.0) + 0.5 # Flow > 0

            next_obs_raw = physics.step(act_rod, act_flow)

            # --- REWARD FUNCTION ---
            reward = 0.0
            power = next_obs_raw[0]

            # 1. Precision Reward (High Gain)
            # exp(-5.0) drops to near zero if power deviates by >20%
            reward += 1.0 * np.exp(-5.0 * abs(power - 1.0))

            # 2. Death Tax (SCRAM Penalty)
            if physics.scram:
                reward -= 500.0
                done = True

            # Store Data
            b_obs.append(obs); b_act.append(action_np); b_logp.append(logp.item())
            b_rew.append(reward); b_val.append(val.item()); b_done.append(float(done))

            # Update State
            if np.isfinite(next_obs_raw).all() and np.max(np.abs(next_obs_raw)) < 100:
                normalizer.update(next_obs_raw)

            obs = normalizer.normalize(next_obs_raw)
            ep_reward += reward

            if done: break

        # --- BATCH PROTECTION ---
        # Skip update if agent died instantly (<5 steps) to prevent math errors
        if len(b_obs) < 5:
            history['reward'].append(ep_reward)
            continue

        # --- PPO UPDATE ---
        b_obs_t = torch.FloatTensor(np.array(b_obs)).to(cfg.DEVICE)
        b_act_t = torch.FloatTensor(np.array(b_act)).to(cfg.DEVICE)
        b_logp_t = torch.FloatTensor(np.array(b_logp)).to(cfg.DEVICE)

        rewards = np.array(b_rew)
        values = np.array(b_val + [0.0])
        returns = np.zeros_like(rewards)
        gae = 0
        for t in reversed(range(len(rewards))):
            delta = rewards[t] + cfg.GAMMA * values[t+1] * (1-b_done[t]) - values[t]
            gae = delta + cfg.GAMMA * cfg.GAE_LAMBDA * (1-b_done[t]) * gae
            returns[t] = gae + values[t]

        b_ret = torch.FloatTensor(returns).to(cfg.DEVICE)
        b_adv = (b_ret - torch.FloatTensor(values[:-1]).to(cfg.DEVICE))

        # Robust Advantage Normalization
        if b_adv.std() > 1e-6:
            b_adv = (b_adv - b_adv.mean()) / (b_adv.std() + 1e-8)
        else:
            b_adv = b_adv - b_adv.mean()

        # Gradient Descent
        for _ in range(cfg.UPDATE_EPOCHS):
            _, new_logp, entropy, new_val = agent.get_action(b_obs_t)
            dist = torch.distributions.Normal(*agent(b_obs_t)[:2])
            new_logp = dist.log_prob(b_act_t).sum(-1)

            ratio = (new_logp - b_logp_t).exp()
            surr1 = ratio * b_adv
            surr2 = torch.clamp(ratio, 1-cfg.CLIP_EPS, 1+cfg.CLIP_EPS) * b_adv

            loss = -torch.min(surr1, surr2).mean() + \
                   0.5 * (new_val.squeeze() - b_ret).pow(2).mean() - \
                   cfg.ENTROPY_COEF * entropy.mean()

            optimizer.zero_grad()
            loss.backward()
            nn.utils.clip_grad_norm_(agent.parameters(), cfg.MAX_GRAD_NORM)
            optimizer.step()

        history['reward'].append(ep_reward)

        if episode % cfg.PRINT_INTERVAL == 0:
            avg_r = np.mean(history['reward'][-cfg.PRINT_INTERVAL:])
            status = "SCRAM" if physics.scram else "STABLE"
            print(f"{episode:5d} | {avg_r:12.2f} | {physics.n:10.3f} | {physics.Tf:8.3f} | {status:>10}")

    return history, agent

# ============================================================
# 6. VISUALIZATION AND VALIDATION
# ============================================================

if __name__ == "__main__":
    hist, trained_agent = train_reactor()

    # 1. Training Performance
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(hist['reward'], color='gray', alpha=0.3)
    plt.plot(np.convolve(hist['reward'], np.ones(20)/20, mode='valid'), color='blue', linewidth=2)
    plt.title("Agent Training Progress")
    plt.xlabel("Episode")
    plt.ylabel("Reward (Max ~2000)")
    plt.grid(True, alpha=0.3)

    # 2. Final Validation Run
    physics = ReactorPhysics()
    norm = RunningMeanStd(shape=(4,))
    obs = norm.normalize(physics.reset())

    data = {'p': [], 'tf': []}
    for _ in range(2000):
        obs_t = torch.FloatTensor(obs).to(cfg.DEVICE).unsqueeze(0)
        with torch.no_grad():
            action, _, _, _ = trained_agent.get_action(obs_t)

        a = action.cpu().numpy()[0]
        obs_raw = physics.step(np.clip(a[0], -1, 1), np.clip(a[1], -1, 1)+0.5)
        obs = norm.normalize(obs_raw)

        data['p'].append(physics.n)
        data['tf'].append(physics.Tf)
        if physics.scram: break

    plt.subplot(1, 2, 2)
    plt.plot(data['p'], label='Power (n)', color='red', linewidth=1.5)
    plt.plot(data['tf'], label='Fuel Temp', color='orange', linestyle='--')
    plt.axhline(1.0, color='black', linestyle=':', label='Target')
    plt.axhline(cfg.MIN_POWER, color='blue', linestyle='--', label='Min Limit')
    plt.axhline(cfg.HARD_LIMIT, color='red', linestyle='--', label='Max Limit')
    plt.title("Final Agent Control Response")
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()
    print("Simulation Complete.")